{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import psutil\n",
    "import chromadb\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Configurazione Logging e Controllo Memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Memoria disponibile: 3.4 GB\n",
      "WARNING:__main__:Bassa memoria! Potrebbero esserci rallentamenti.\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def check_memory():\n",
    "    available_memory = psutil.virtual_memory().available / (1024 ** 3)\n",
    "    logger.info(f\"Memoria disponibile: {available_memory:.1f} GB\")\n",
    "    if available_memory < 4.0:\n",
    "        logger.warning(\"Bassa memoria! Potrebbero esserci rallentamenti.\")\n",
    "\n",
    "check_memory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento e Preprocessing dei Documenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Caricati 1674 chunk da 4 file.\n"
     ]
    }
   ],
   "source": [
    "def load_documents(docs_folder=\"documents\"):\n",
    "    all_documents = []\n",
    "    \n",
    "    for file in os.listdir(docs_folder):\n",
    "        file_path = os.path.join(docs_folder, file)\n",
    "\n",
    "        if file.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file.endswith(\".txt\"):\n",
    "            loader = TextLoader(file_path)\n",
    "        else:\n",
    "            continue  # Ignora file non supportati\n",
    "\n",
    "        documents = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "        # Aggiunta metadati con nome file e tipo di dato\n",
    "        for chunk in chunks:\n",
    "            chunk.metadata[\"file\"] = file\n",
    "            chunk.metadata[\"type\"] = \"text\"\n",
    "\n",
    "        all_documents.extend(chunks)\n",
    "\n",
    "    logger.info(f\"Caricati {len(all_documents)} chunk da {len(os.listdir(docs_folder))} file.\")\n",
    "    return all_documents\n",
    "\n",
    "documents = load_documents()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione del Database Vettoriale (ChromaDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:__main__:Database ChromaDB creato con successo.\n"
     ]
    }
   ],
   "source": [
    "def create_vectorstore(documents, db_path=\"chromadb_index\"):\n",
    "    chroma_client = chromadb.PersistentClient(path=db_path)\n",
    "    collection = chroma_client.get_or_create_collection(name=\"rag_collection\")\n",
    "\n",
    "    for i, doc in enumerate(documents):\n",
    "        collection.add(\n",
    "            documents=[doc.page_content], \n",
    "            metadatas=[doc.metadata], \n",
    "            ids=[str(i)]\n",
    "        )\n",
    "\n",
    "    logger.info(\"Database ChromaDB creato con successo.\")\n",
    "    return chroma_client, collection\n",
    "\n",
    "chroma_client, collection = create_vectorstore(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esportazione dei Chunk in un File di Testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Esportazione completata: chromadb_index\\db_export.txt\n"
     ]
    }
   ],
   "source": [
    "def export_chunks():\n",
    "    chroma_client = chromadb.PersistentClient(path=db_path)\n",
    "    collection = chroma_client.get_collection(name=\"rag_collection\")\n",
    "    results = collection.get(include=[\"documents\", \"metadatas\"])\n",
    "\n",
    "    export_path = os.path.join(db_path, \"db_export.txt\")\n",
    "    \n",
    "    with open(export_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for doc, meta in zip(results[\"documents\"], results[\"metadatas\"]):\n",
    "            file.write(f\"FILE: {meta['file']}\\n\")\n",
    "            file.write(f\"TIPO: {meta['type']}\\n\")\n",
    "            file.write(f\"CONTENUTO:\\n{doc}\\n\")\n",
    "            file.write(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "    logger.info(f\"Esportazione completata: {export_path}\")\n",
    "\n",
    "export_chunks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query del Database e Generazione Risposte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "model_name = \"llama3.2\"\n",
    "llm = ChatOllama(model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funzione per Recupero Dati dal Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chromadb(query_text, n_results=3, db_path=\"chromadb_index\"):\n",
    "    chroma_client = chromadb.PersistentClient(path=db_path)\n",
    "    collection = chroma_client.get_collection(name=\"rag_collection\")\n",
    "    results = collection.query(query_texts=[query_text], n_results=n_results)\n",
    "    \n",
    "    # Appiana eventuali liste annidate\n",
    "    retrieved_docs = [doc for sublist in results[\"documents\"] for doc in sublist]\n",
    "    return retrieved_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione del Prompt e Generazione Risposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question:\n",
    "If you don't know the answer, then do not answer from your own knowledge.\n",
    "Keep the answer concise.\n",
    "\n",
    "#### Retrieved Context ####\n",
    "{context}\n",
    "\n",
    "#### Question ####\n",
    "{question}\n",
    "\n",
    "#### LLM Response ####\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def ask_question(question):\n",
    "    retrieved_docs = query_chromadb(question)\n",
    "\n",
    "    if not retrieved_docs:\n",
    "        return \"Nessuna informazione trovata nel database.\"\n",
    "\n",
    "    # Unisce i documenti in un'unica stringa\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "    # Genera la risposta con il modello AI\n",
    "    response = prompt.invoke({\"context\": context, \"question\": question})\n",
    "    response = llm.invoke(response)\n",
    "    response = StrOutputParser().invoke(response)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfaccia da Terminale per Interrogare il Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have a question to answer. You can ask me anything, and I'll do my best to help!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, you want to know about special computer chips called GPUs.\n",
      "\n",
      "GPU stands for Graphics Processing Unit. It's like a superhero for computers that helps them play games and show pretty pictures on the screen.\n",
      "\n",
      "Imagine your computer is like a big brain, and it needs help with some of its tasks. That's where the GPU comes in! It's super good at doing lots of little math problems at the same time, which makes games and videos look really cool.\n",
      "\n",
      "There are different kinds of GPUs, but some of the biggest companies that make them are Nvidia, AMD, and Intel. They're always trying to make their GPUs better and faster so they can help computers do more amazing things!\n",
      "\n",
      "That's it!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems you've forgot to ask a question! Could you please provide a question for me to answer? I'll do my best to assist you.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        query = input(\"Inserisci una domanda (o 'exit' per uscire): \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "        print(ask_question(query))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_prove)",
   "language": "python",
   "name": "rag_prove"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
