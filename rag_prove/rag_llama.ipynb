{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import psutil\n",
    "import chromadb\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "\n",
    "    def __init__(self, model_name=\"llama3.2\", embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\", docs_folder=\"documents\"):\n",
    "        self.setup_logging()\n",
    "        self.check_memory()\n",
    "        self.model_name = model_name\n",
    "        self.embedding_model = embedding_model\n",
    "        self.docs_folder = docs_folder\n",
    "\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=self.embedding_model)\n",
    "        self.llm = ChatOllama(model=self.model_name)\n",
    "\n",
    "        self.documents = self.load_documents()\n",
    "        self.vectorstore = self.create_vectorstore()\n",
    "        self.rag_chain = self.setup_rag_chain()\n",
    "\n",
    "    #############################\n",
    "\n",
    "    def setup_logging(self):\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    #############################\n",
    "\n",
    "    def check_memory(self):\n",
    "        available_memory = psutil.virtual_memory().available / (1024 ** 3)\n",
    "        self.logger.info(f\"Memoria disponibile: {available_memory:.1f} GB\")\n",
    "        if available_memory < 4.0:\n",
    "            self.logger.warning(\"Bassa memoria! Potrebbero esserci rallentamenti.\")\n",
    "    \n",
    "    #############################\n",
    "\n",
    "    def load_documents(self):\n",
    "        all_documents = []\n",
    "        for file in os.listdir(self.docs_folder):\n",
    "            file_path = os.path.join(self.docs_folder, file)\n",
    "\n",
    "            if file.endswith(\".pdf\"):\n",
    "                loader = PyPDFLoader(file_path)\n",
    "            elif file.endswith(\".txt\"):\n",
    "                loader = TextLoader(file_path)\n",
    "            else:\n",
    "                continue  # Ignora file non supportati\n",
    "\n",
    "            documents = loader.load()\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "            chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "            # Aggiunta metadati con nome file e tipo di dato\n",
    "            for chunk in chunks:\n",
    "                chunk.metadata[\"file\"] = file\n",
    "                chunk.metadata[\"type\"] = \"text\"\n",
    "\n",
    "            all_documents.extend(chunks)\n",
    "        \n",
    "        self.logger.info(f\"Caricati {len(all_documents)} chunk da {len(os.listdir(self.docs_folder))} file.\")\n",
    "        return all_documents\n",
    "    \n",
    "    #############################\n",
    "\n",
    "    def create_vectorstore(self):\n",
    "        chroma_client = chromadb.PersistentClient(path=\"chromadb_index\")\n",
    "        collection = chroma_client.get_or_create_collection(name=\"rag_collection\")\n",
    "\n",
    "        for i, doc in enumerate(self.documents):\n",
    "            collection.add(\n",
    "                documents=[doc.page_content], \n",
    "                metadatas=[doc.metadata], \n",
    "                ids=[str(i)]\n",
    "            )\n",
    "\n",
    "        return collection\n",
    "    \n",
    "    #############################\n",
    "\n",
    "    def setup_rag_chain(self):\n",
    "        def chromadb_retriever(query_text):\n",
    "            results = self.vectorstore.query(query_texts=[query_text], n_results=3)\n",
    "            return results[\"documents\"]\n",
    "\n",
    "        template = \"\"\"\n",
    "        You are an assistant for question-answering tasks.\n",
    "        Use the following pieces of retrieved context to answer the question:\n",
    "        If you don't know the answer, then do not answer from your own knowledge.\n",
    "        Keep the answer concise.\n",
    "        \n",
    "        #### Retrieved Context ####\n",
    "        {context}\n",
    "        \n",
    "        #### Question ####\n",
    "        {question}\n",
    "        \n",
    "        #### LLM Response ####\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "        return (\n",
    "            {\"context\": chromadb_retriever, \"question\": lambda x: x} \n",
    "            | prompt \n",
    "            | self.llm \n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    \n",
    "    #############################\n",
    "\n",
    "    def query(self, question):\n",
    "        memory_usage = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024\n",
    "        self.logger.info(f\"Uso memoria: {memory_usage:.1f} MB\")\n",
    "        return self.rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Memoria disponibile: 1.1 GB\n",
      "WARNING:__main__:Bassa memoria! Potrebbero esserci rallentamenti.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:__main__:Caricati 1674 chunk da 4 file.\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:__main__:Uso memoria: 669.5 MB\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have any context for the question \"Hi!\". Can you please provide more information or clarify what you are asking?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Uso memoria: 23.1 MB\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have enough context to answer this question.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Uso memoria: 92.7 MB\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapley values were first introduced by L. Shapley in the context of game theory and are used to explain the predictions of ML models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Uso memoria: 94.2 MB\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have enough context to provide an answer. Please provide more information or clarify the question so I can assist you better.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    rag = RAGPipeline()  # Evitiamo parametri ridondanti\n",
    "\n",
    "    while True:\n",
    "        query = input(\"Inserisci una domanda (o 'exit' per uscire): \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "        print(rag.query(query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_prove)",
   "language": "python",
   "name": "rag_prove"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
