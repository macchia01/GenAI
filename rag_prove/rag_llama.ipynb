{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import psutil\n",
    "import chromadb\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Configurazione Logging e Controllo Memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Memoria disponibile: 0.6 GB\n",
      "WARNING:__main__:Bassa memoria! Potrebbero esserci rallentamenti.\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def check_memory():\n",
    "    available_memory = psutil.virtual_memory().available / (1024 ** 3)\n",
    "    logger.info(f\"Memoria disponibile: {available_memory:.1f} GB\")\n",
    "    if available_memory < 4.0:\n",
    "        logger.warning(\"Bassa memoria! Potrebbero esserci rallentamenti.\")\n",
    "\n",
    "check_memory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento e Preprocessing dei Documenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Caricati 1674 chunk da 4 file.\n"
     ]
    }
   ],
   "source": [
    "def load_documents(docs_folder=\"documents\"):\n",
    "    all_documents = []\n",
    "    \n",
    "    for file in os.listdir(docs_folder):\n",
    "        file_path = os.path.join(docs_folder, file)\n",
    "\n",
    "        if file.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file.endswith(\".txt\"):\n",
    "            loader = TextLoader(file_path)\n",
    "        else:\n",
    "            continue  # Ignora file non supportati\n",
    "\n",
    "        documents = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "        # Aggiunta metadati con nome file e tipo di dato\n",
    "        for chunk in chunks:\n",
    "            chunk.metadata[\"file\"] = file\n",
    "            chunk.metadata[\"type\"] = \"text\"\n",
    "\n",
    "        all_documents.extend(chunks)\n",
    "\n",
    "    logger.info(f\"Caricati {len(all_documents)} chunk da {len(os.listdir(docs_folder))} file.\")\n",
    "    return all_documents\n",
    "\n",
    "documents = load_documents()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione del Database Vettoriale (ChromaDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Database ChromaDB creato con successo.\n"
     ]
    }
   ],
   "source": [
    "def create_vectorstore(documents, db_path=\"chromadb_index\"):\n",
    "    chroma_client = chromadb.PersistentClient(path=db_path)\n",
    "    collection = chroma_client.get_or_create_collection(name=\"rag_collection\")\n",
    "\n",
    "    for i, doc in enumerate(documents):\n",
    "        collection.add(\n",
    "            documents=[doc.page_content], \n",
    "            metadatas=[doc.metadata], \n",
    "            ids=[str(i)]\n",
    "        )\n",
    "\n",
    "    logger.info(\"Database ChromaDB creato con successo.\")\n",
    "    return chroma_client, collection\n",
    "\n",
    "chroma_client, collection = create_vectorstore(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esportazione dei Chunk in un File di Testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m             file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEsportazione completata: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexport_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mexport_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m, in \u001b[0;36mexport_chunks\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexport_chunks\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     chroma_client \u001b[38;5;241m=\u001b[39m chromadb\u001b[38;5;241m.\u001b[39mPersistentClient(path\u001b[38;5;241m=\u001b[39m\u001b[43mdb_path\u001b[49m)\n\u001b[0;32m      3\u001b[0m     collection \u001b[38;5;241m=\u001b[39m chroma_client\u001b[38;5;241m.\u001b[39mget_collection(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag_collection\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     results \u001b[38;5;241m=\u001b[39m collection\u001b[38;5;241m.\u001b[39mget(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadatas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'db_path' is not defined"
     ]
    }
   ],
   "source": [
    "def export_chunks():\n",
    "    chroma_client = chromadb.PersistentClient(path=db_path)\n",
    "    collection = chroma_client.get_collection(name=\"rag_collection\")\n",
    "    results = collection.get(include=[\"documents\", \"metadatas\"])\n",
    "\n",
    "    export_path = os.path.join(db_path, \"db_export.txt\")\n",
    "    \n",
    "    with open(export_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for doc, meta in zip(results[\"documents\"], results[\"metadatas\"]):\n",
    "            file.write(f\"FILE: {meta['file']}\\n\")\n",
    "            file.write(f\"TIPO: {meta['type']}\\n\")\n",
    "            file.write(f\"CONTENUTO:\\n{doc}\\n\")\n",
    "            file.write(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "    logger.info(f\"Esportazione completata: {export_path}\")\n",
    "\n",
    "export_chunks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query del Database e Generazione Risposte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "model_name = \"llama3.2\"\n",
    "llm = ChatOllama(model=model_name, stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funzione per Recupero Dati dal Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chromadb(query_text, n_results=3, db_path=\"chromadb_index\"):\n",
    "    chroma_client = chromadb.PersistentClient(path=db_path)\n",
    "    collection = chroma_client.get_collection(name=\"rag_collection\")\n",
    "    results = collection.query(query_texts=[query_text], n_results=n_results)\n",
    "    \n",
    "    # Appiana eventuali liste annidate\n",
    "    retrieved_docs = [doc for sublist in results[\"documents\"] for doc in sublist]\n",
    "    return retrieved_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione del Prompt e Generazione Risposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question:\n",
    "If you don't know the answer, then do not answer from your own knowledge.\n",
    "Keep the answer concise.\n",
    "\n",
    "#### Retrieved Context ####\n",
    "{context}\n",
    "\n",
    "#### Question ####\n",
    "{question}\n",
    "\n",
    "#### LLM Response ####\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def ask_question(question):\n",
    "    retrieved_docs = query_chromadb(question)\n",
    "\n",
    "    if not retrieved_docs:\n",
    "        return \"Nessuna informazione trovata nel database.\"\n",
    "\n",
    "    # Unisce i documenti in un'unica stringa\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "    # Genera la risposta con il modello AI\n",
    "    response = prompt.invoke({\"context\": context, \"question\": question})\n",
    "    response = llm.invoke(response)\n",
    "    response = StrOutputParser().invoke(response)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfaccia da Terminale per Interrogare il Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have information on \"cosmic rays\" from the retrieved context.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        query = input(\"Inserisci una domanda (o 'exit' per uscire): \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "        print(ask_question(query))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_prove)",
   "language": "python",
   "name": "rag_prove"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
